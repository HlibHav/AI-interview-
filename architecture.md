# System architecture for the AI‑interview assistant

This document describes the architectural blueprint for the hackathon project.
It complements the **technical design** by focusing on component boundaries,
data flows and infrastructure concerns.  The architecture is deliberately
modular so that individual services (e.g., vector store, LLM observability)
can be swapped or scaled independently.

## 1. Component overview

```
┌────────────────────────────┐     ┌────────────────────────────┐
│        Respondent UI       │     │         Admin UI           │
│ (Next.js + WebRTC/HTTP)    │     │ (Next.js dashboard)        │
└──────────────┬─────────────┘     └──────────────┬─────────────┘
               │                                  │
               │                                  │
    Audio/video stream & events                   │
               ▼                                  │
       ┌───────────────────────────┐              │
       │     Session service       │<─────────────┘
       │ (FastAPI/Node.js backend) │
       ├───────────────────────────┤
       │ LLM agent orchestrator    │
       │  – Clarification agent    │
       │  – Planner agent          │
       │  – Interviewer agent      │
       │  – Summariser agent       │
       │  – Psychometric agent     │
       ├───────────────────────────┤
       │ Beyond Presence client    │
       │ (managed audio/video)     │
       ├───────────────────────────┤
       │ Weaviate client           │
       │ Phoenix instrumentation   │
       └───────┬───────────────────┘
               │
               │
     ┌─────────▼─────────┐    ┌─────────────────────────┐
     │  Weaviate vector   │    │   Phoenix server        │
     │      database      │    │ (self‑hosted or cloud)  │
     └────────────────────┘    └─────────────────────────┘
```

### Front end

* **Respondent UI** – a web application where participants start the
  interview.  It integrates the **Beyond Presence** SDK to acquire
  microphone and camera permissions, stream audio to the managed agent
  and render a hyper‑realistic avatar with lip‑synced video【413673914178042†L23-L39】.
  The UI allows pausing/resuming, displays progress indicators and falls
  back to audio‑only mode if the avatar service is unavailable.
* **Admin UI** – a dashboard for researchers to enter the research goal,
  answer clarifying questions, review and edit the interview plan and view
  results.  It displays transcripts, summaries, psychometric profiles and
  Phoenix evaluation metrics.

### Session service

The back end orchestrates the interview lifecycle:

1. **Session creation** – when an admin approves a script, the service
   generates a unique session ID and URL for the respondent.  It stores
   session metadata (goal, script ID, admin email) in Weaviate.
2. **Streaming** – once a respondent joins, the session service opens a
   streaming connection with **Beyond Presence’s managed agent**.  Audio from
   the respondent is forwarded to Beyond Presence, which handles
   speech‑to‑text and returns transcripts along with synchronised audio/video
   responses【413673914178042†L23-L39】.  The service sends the LLM’s replies
   (generated by the interviewer agent) to Beyond Presence for rendering.
3. **Agent orchestration** – the orchestrator coordinates multiple agents:
   * **Clarification agent** interacts with the admin until the research
     brief is complete.  Its prompts and answers are stored.
   * **Planner agent** generates the script from the brief and stores it.
   * **Interviewer agent** drives the conversation, selects follow‑ups and
     determines when to end the interview.  It uses memory from Weaviate and
     receives summarised transcripts from the summariser.
   * **Summariser agent** compresses each respondent answer into a concise
     summary and stores the chunk in Weaviate.  It also updates the Phoenix
     trace with semantic attributes.
   * **Psychometric agent** runs after the session, analysing the entire
     transcript to produce Big‑Five scores and an Enneagram estimate.  Results
     are stored and visualised on the admin dashboard.
4. **Instrumenting with Phoenix** – all agent interactions and function
   calls are wrapped with Phoenix’s OpenTelemetry instrumentation.  The
   `phoenix.otel.register` call sets up the tracer, and decorators (`@tracer.chain`)
   are applied to key functions.  The service exports spans to the Phoenix
   server for analysis.  Evaluation routines may be triggered after each
   question to score answer relevance.

### Data storage and retrieval

* **Weaviate** – used as the long‑term memory store.  It holds research
  goals, interview plans, transcript chunks, summaries, psychometric profiles
  and evaluation scores.  Data are stored as objects with vector embeddings
  for efficient similarity search.  The orchestrator queries Weaviate via the
  `nearText` or `hybrid` API when generating follow‑ups or summarising.
* **Phoenix** – stores observability data: traces, spans, evaluation
  results and experiment metadata.  Its UI allows developers to visualise
  call graphs and measure the quality of LLM outputs.

## 2. Workflow sequence

The end‑to‑end flow proceeds as follows:

1. **Goal definition:** the admin enters a research goal in the dashboard.
2. **Clarification loop:** the clarification agent asks follow‑up questions
   until the brief is complete.  The conversation is stored in Weaviate and
   traced in Phoenix.
3. **Plan generation:** the planner agent produces a structured script.
   Admins can revise the script; modifications are versioned in Weaviate.
4. **Link distribution:** once approved, the system issues a link to the
   respondent and awaits their connection.
5. **Interview session:** the respondent joins and streams audio via
   Beyond Presence.  The managed agent performs speech‑to‑text and returns
   transcripts to the session service.  The interviewer agent runs the
   conversation loop: ask a question, summarise the answer and decide the
   next prompt.  Generated replies are sent to Beyond Presence, which
   returns synchronised audio/video for the participant.  Each step
   produces spans exported to Phoenix.
6. **Summarisation and profiling:** after each answer the summariser and
   psychometric agents update Weaviate.  At the end, a final summary and
   personality profile are produced.
7. **Analysis and evaluation:** researchers view transcripts, summaries,
   psychometric radar charts and Phoenix evaluation metrics in the admin
   dashboard.  Cross‑session queries (e.g., “what themes recur across all
   flirting‑habit interviews?”) can be run via Weaviate’s generative search.

## 3. Future extensions

* **Scaling:** microservices can be separated (e.g., avatar/streaming
  service, vector store, LLM orchestrator) and deployed independently.
  Phoenix supports multi‑tenant deployments so evaluation metrics remain
  segregated.
* **Continuous improvement:** integrate CI/CD pipelines that run Phoenix
  evaluations on new prompt versions.  Use the results to choose the best
  performing configuration automatically.
* **Ethics and compliance:** add a data‑privacy service to handle
  anonymisation and consent tracking.  Logs and traces sent to Phoenix should
  exclude personally identifiable information.
